{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7ed374",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b67e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "###document structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e024cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Ansuman Pandey', 'date_created': '2025-10-21'}, page_content='this is the main text context I am using to create a RAG')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text context I am using to create a RAG\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Ansuman Pandey\",\n",
    "        \"date_created\":\"2025-10-21\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03165eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/test_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d166137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34564e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ed6aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be0b340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Databricks Interview Playbook - Full Edition', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'trapped': '', 'modDate': \"D:20251024162625+00'00'\", 'creationDate': \"D:20251024162625+00'00'\", 'page': 0}, page_content='I Databricks Interview Playbook\\nFor Experienced Data Professionals (Banking & Retail Domain)\\nComprehensive Q&A; Guide + Quick Reference for Senior-Level Interviews\\n1. Databricks Core Concepts\\nQ: What is Databricks and why is it used?\\nA: Databricks is a unified analytics and AI platform built on Apache Spark that combines data\\nengineering, data science, and machine learning. In my project, we use it as a central data\\nprocessing layer to transform raw banking transactions and customer data into curated,\\nanonymized datasets for analytics.\\nQ: How does Databricks differ from traditional Spark?\\nA: Databricks provides managed clusters, collaborative notebooks, built-in Delta Lake, and\\noptimized I/O performance, improving reliability compared to self-managed Spark.\\nQ: What is Unity Catalog and its role?\\nA: Unity Catalog provides centralized governance — access control, data lineage, and audit\\nlogging — across workspaces. We used it to manage permissions on de-identified retail data and\\ntrack lineage across DBT models and reporting schemas.\\n2. Apache Spark on Databricks\\nQ: How do you optimize Spark jobs in Databricks?\\nA: Adjusted shuffle partitions, used broadcast joins for smaller tables, persisted intermediate data,\\nand monitored via Spark UI.\\nQ: What’s data skew and how do you handle it?\\nA: Occurs when certain keys have disproportionate data. Resolved using salting, repartitioning, or\\nskew join hints.\\nQ: Difference between narrow and wide transformations?\\nA: Narrow (map, filter) don’t require shuffles; wide (join, groupBy) do. Optimizing these is key for\\nlarge datasets like transactions.\\nQ: What are the Catalyst Optimizer and Tungsten?\\nA: Catalyst performs query optimization (logical and physical plans). Tungsten handles in-memory\\ncomputation and code generation for performance gains.\\n3. Delta Lake\\nQ: What is Delta Lake and why is it important?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Databricks Interview Playbook - Full Edition', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'trapped': '', 'modDate': \"D:20251024162625+00'00'\", 'creationDate': \"D:20251024162625+00'00'\", 'page': 1}, page_content='A: Delta Lake brings ACID transactions, schema enforcement, and versioning to data lakes —\\nensuring reliability in banking data.\\nQ: Explain time travel in Delta Lake.\\nA: Query older versions of data using timestamps or version numbers — helpful for audit and\\nrollback.\\nQ: How do you perform incremental loads?\\nA: Used MERGE INTO with watermark logic on timestamps to process only new or updated\\nrecords.\\nQ: What’s OPTIMIZE and ZORDER?\\nA: OPTIMIZE compacts small files; ZORDER colocates related data (e.g., by customer_id or\\nregion) for better performance.\\nQ: How do you handle schema evolution?\\nA: Enabled automatic schema evolution and used mergeSchema in writes to support changing data\\nstructures.\\n4. Medallion Architecture (Bronze–Silver–Gold)\\nQ: Explain the Medallion architecture.\\nA: Bronze: raw ingestion; Silver: cleaned and conformed data (via DBT); Gold: aggregated,\\nanonymized business views.\\nQ: How do you ensure data quality?\\nA: DBT tests and Databricks expectations validate data integrity. Alerts trigger on failures.\\nQ: How do you handle schema drift?\\nA: Enabled automatic schema evolution in Delta and tracked metadata changes via Unity Catalog.\\nQ: How do you implement incremental pipelines?\\nA: Used Delta change data feed (CDF) and MERGE logic to propagate only changed data between\\nlayers.\\n5. Performance & Cost Optimization\\nQ: How do you optimize Databricks jobs for cost and speed?\\nA: Used auto-scaling clusters, auto-termination, caching, and efficient partitioning.\\nQ: How do you handle large joins efficiently?\\nA: Broadcasted smaller reference data, optimized shuffle partitions, and used ZORDER for\\nco-location.\\nQ: How do you monitor performance?\\nA: Used Databricks Ganglia metrics, Spark UI, and job run history to analyze stages, shuffles, and\\ntask skew.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Databricks Interview Playbook - Full Edition', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'trapped': '', 'modDate': \"D:20251024162625+00'00'\", 'creationDate': \"D:20251024162625+00'00'\", 'page': 2}, page_content='6. Integrations & Orchestration\\nQ: How do you orchestrate data pipelines?\\nA: DBT handles SQL-based transformations, orchestrated via Airflow with Databricks Jobs.\\nQ: How do you integrate Databricks with BI tools?\\nA: Exposed Gold tables as Delta Live Tables or SQL endpoints for Power BI dashboards.\\nQ: How do you use Git in Databricks?\\nA: Used Databricks Repos with GitHub for CI/CD and model versioning.\\nQ: How do you handle dependencies between jobs?\\nA: Used Databricks Jobs API and Airflow DAGs to maintain execution order and dependencies.\\n7. Real-World Project Scenarios\\nQ: Describe an end-to-end pipeline you built.\\nA: Raw retail transactions ingested to Bronze; DBT transformations create Silver; Gold contains\\ndaily spend, KPIs, and de-identified analytics.\\nQ: How did you manage data privacy?\\nA: Applied masking and tokenization via DBT macros before Gold; restricted raw access via Unity\\nCatalog.\\nQ: What’s a challenge you solved in Databricks?\\nA: Solved small-file performance degradation with OPTIMIZE, ZORDER, and better partitioning.\\nQ: How do you ensure regulatory compliance?\\nA: Implemented lineage through Unity Catalog and maintained audit-ready Delta version history.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Databricks Interview Playbook - Full Edition', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'trapped': '', 'modDate': \"D:20251024162625+00'00'\", 'creationDate': \"D:20251024162625+00'00'\", 'page': 3}, page_content=\"I Bonus Quick Reference: Databricks Commands &\\nOptimization Tips\\n1. Common Delta Lake SQL Commands\\nCommand / Topic\\nExample / Description\\nCreate Delta Table\\nCREATE TABLE sales_delta USING DELTA AS SELECT * FROM parquet.`/mnt/data/sales/`;\\nUpsert (Merge)\\nMERGE INTO target t USING updates u ON t.id = u.id \\nWHEN MATCHED THEN UPDATE SET * \\nWHEN NOT MATCHED THEN INSERT *;\\nTime Travel Query\\nSELECT * FROM sales_delta VERSION AS OF 3;\\nOptimize & ZORDER\\nOPTIMIZE sales_delta ZORDER BY (customer_id);\\nVacuum\\nVACUUM sales_delta RETAIN 168 HOURS;\\n2. Useful PySpark Commands\\nCommand / Topic\\nExample / Description\\nRead Delta Table\\ndf = spark.read.format('delta').load('/mnt/data/sales_delta')\\nWrite Delta Table\\ndf.write.format('delta').mode('overwrite').save('/mnt/data/sales_delta')\\nRepartition Data\\ndf = df.repartition(50)\\nCache Table\\ndf.cache()\\nDisplay Data (Databricks)\\ndisplay(df)\\n3. Performance Optimization Tips\\nCommand / Topic\\nExample / Description\\nPartition Strategy\\nPartition large Delta tables by logical keys (e.g., region, month).\\nSmall File Problem\\nUse OPTIMIZE regularly to compact files.\\nBroadcast Joins\\nBroadcast small lookup tables using broadcast(df).\\nShuffle Optimization\\nSet spark.sql.shuffle.partitions based on data volume (e.g., 200–400).\\nSchema Evolution\\nEnable mergeSchema option for evolving data sources.\\n4. Data Governance & Security\\nCommand / Topic\\nExample / Description\\nUnity Catalog Permissions\\nGRANT SELECT ON TABLE gold.transactions TO analyst_role;\\nMask Sensitive Columns\\nUse DBT macros or CASE statements to hash/mask PII.\\nAudit Trail\\nUse Delta’s version history and Unity Catalog lineage for audits.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Spark Interview Playbook - Full Version', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'trapped': '', 'modDate': \"D:20251024175657+00'00'\", 'creationDate': \"D:20251024175657+00'00'\", 'page': 0}, page_content='I Spark Interview Playbook (Detailed Version)\\nContext: Banking & Retail Data Pipelines with DBT & Databricks\\nI How Spark Fits into the Banking & Retail Data Pipeline\\nData Sources (Core Banking, Retail POS, CRM, Cards) → Bronze (Raw Landing via Ingestion) → Silver\\n(Cleansed & Transformed using Spark + DBT) → Gold (Aggregated & De-identified Reporting) → Business\\nDashboards & Analytics.\\n1. Spark Core & Architecture\\nQ: How does Spark fit into your pipeline?\\nA: Spark executes the transformations defined in DBT or Databricks notebooks. It processes large volumes of\\ntransactional and customer data in a distributed manner, leveraging cluster computing.\\nQ: How does Spark handle distributed processing?\\nA: Spark splits data into partitions and processes them in parallel across executors, enabling\\nhigh-performance distributed computation.\\nQ: What Spark components do you use?\\nA: Spark SQL for structured transformations, DataFrame API for aggregations, and Structured Streaming for\\nincremental or event-driven updates.\\n2. Transformations, Actions & Use Cases\\nQ: Explain narrow vs. wide transformations.\\nA: Narrow transformations (e.g., filter, map) operate within partitions, while wide transformations (e.g.,\\ngroupBy, join) trigger shuffles across nodes.\\nQ: How do you manage large joins?\\nA: Used broadcast joins for small lookup tables, repartitioned large tables, and leveraged Delta ZORDER for\\njoin optimization.\\nQ: How do you address data skew?\\nA: Added salting keys, used skew hints, or repartitioned by balanced keys to handle uneven data distribution.\\n3. Performance Tuning & Shuffle Optimization\\nQ: Which configurations do you tune for performance?\\nA: Adjusted spark.sql.shuffle.partitions, spark.default.parallelism, and enabled autoBroadcastJoinThreshold\\nfor optimized shuffles.\\nQ: How do you handle small file issues?\\nA: Used coalesce/repartition, Delta OPTIMIZE, and periodic compaction jobs.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Spark Interview Playbook - Full Version', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'trapped': '', 'modDate': \"D:20251024175657+00'00'\", 'creationDate': \"D:20251024175657+00'00'\", 'page': 1}, page_content='Q: How do you cache and persist data?\\nA: Cached frequently reused intermediate DataFrames; persisted key datasets across multiple DBT model\\nruns.\\n4. Schema Handling, Incremental Loads & Data Quality\\nQ: How do you handle schema drift?\\nA: Enabled mergeSchema for Delta tables and validated schema changes in DBT before production runs.\\nQ: How do you manage nulls and bad records?\\nA: Applied na.drop(), na.fill(), and UDF-based cleaning; directed bad records to a quarantine path for review.\\nQ: How do you perform incremental loads?\\nA: Implemented watermark-based ingestion, and Delta MERGE INTO to efficiently update only changed data.\\n5. Spark + DBT + Databricks Integration\\nQ: How do DBT models interact with Spark?\\nA: DBT compiles SQL logic that runs on Spark’s engine in Databricks. Incremental DBT models are optimized\\nusing Spark SQL for large-scale updates.\\nQ: How do you orchestrate Spark jobs?\\nA: Used Airflow or Databricks Workflows to trigger Spark notebooks and DBT models with dependency\\nmanagement.\\nQ: How do you ensure reusability?\\nA: Parameterized transformations and used modular DBT macros for shared logic across multiple Spark jobs.\\n6. Monitoring, Debugging & Real-World Scenarios\\nQ: How do you monitor Spark job performance?\\nA: Used Databricks Spark UI for stage analysis, task skew, and shuffle metrics; tracked job run durations in\\nDatabricks Job metrics.\\nQ: Example of a performance optimization you implemented?\\nA: Optimized a large aggregation from 3 hours to 45 minutes by reducing shuffle partitions, broadcasting small\\ntables, and caching intermediate data.\\nQ: How do you debug failing Spark stages?\\nA: Checked executor logs in the Spark UI, reviewed lineage in Unity Catalog, and reran partitions with lower\\nconcurrency to isolate bad data.\\n7. Governance, Security & Compliance\\nQ: How do you handle confidential data?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Spark Interview Playbook - Full Version', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'trapped': '', 'modDate': \"D:20251024175657+00'00'\", 'creationDate': \"D:20251024175657+00'00'\", 'page': 2}, page_content='A: Masked or hashed customer identifiers and ensured data encryption at rest; enforced table-level\\npermissions via Unity Catalog.\\nQ: How do you ensure lineage and traceability?\\nA: Delta Lake version history and Unity Catalog lineage tracking provide full visibility of source →\\ntransformation → output.\\nQ: What best practices do you follow for production pipelines?\\nA: Use job clusters, CI/CD deployments, parameterized notebooks, and automated validation checks in DBT +\\nSpark.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Spark Interview Playbook - Full Version', 'author': '(anonymous)', 'subject': '(unspecified)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'trapped': '', 'modDate': \"D:20251024175657+00'00'\", 'creationDate': \"D:20251024175657+00'00'\", 'page': 3}, page_content=\"I Bonus Quick Reference: Spark Commands & Optimization Tips\\nTopic\\nExample / Command / Description\\nRead & Write Delta\\nspark.read.format('delta').load('/mnt/sales'); df.write.format('delta').save('/mnt/output')\\nBroadcast Join\\ndf_large.join(broadcast(df_small), 'id')\\nShuffle Partitions\\nspark.conf.set('spark.sql.shuffle.partitions', 400)\\nHandle Skew\\ndf.repartition('region') or use salting technique\\nOptimize Table\\nOPTIMIZE gold.sales ZORDER BY (customer_id)\\nCache Data\\ndf.cache(); df.count()\\nSchema Evolution\\ndf.write.option('mergeSchema', 'true').format('delta').save(path)\\nIncremental Load\\nMERGE INTO gold USING updates ON keys WHEN MATCHED THEN UPDATE WHEN NOT\\nThese commands and best practices are ideal for Spark-based transformations within Databricks + DBT\\npipelines in a secure, governed banking environment.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
