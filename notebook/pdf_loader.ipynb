{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3303cf8",
   "metadata": {},
   "source": [
    "RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader # type: ignore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # type: ignore\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc92832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: Databricks_Interview_Playbook_Full.pdf\n",
      "  ✓ Loaded 4 pages\n",
      "\n",
      "Processing: Spark_Interview_Playbook_Full.pdf\n",
      "  ✓ Loaded 4 pages\n",
      "\n",
      "Total documents loaded: 8\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa3039ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='■ Databricks Interview Playbook\\nFor Experienced Data Professionals (Banking & Retail Domain)\\nComprehensive Q&A; Guide + Quick Reference for Senior-Level Interviews\\n1. Databricks Core Concepts\\nQ: What is Databricks and why is it used?\\nA: Databricks is a unified analytics and AI platform built on Apache Spark that combines data\\nengineering, data science, and machine learning. In my project, we use it as a central data\\nprocessing layer to transform raw banking transactions and customer data into curated,\\nanonymized datasets for analytics.\\nQ: How does Databricks differ from traditional Spark?\\nA: Databricks provides managed clusters, collaborative notebooks, built-in Delta Lake, and\\noptimized I/O performance, improving reliability compared to self-managed Spark.\\nQ: What is Unity Catalog and its role?\\nA: Unity Catalog provides centralized governance — access control, data lineage, and audit\\nlogging — across workspaces. We used it to manage permissions on de-identified retail data and\\ntrack lineage across DBT models and reporting schemas.\\n2. Apache Spark on Databricks\\nQ: How do you optimize Spark jobs in Databricks?\\nA: Adjusted shuffle partitions, used broadcast joins for smaller tables, persisted intermediate data,\\nand monitored via Spark UI.\\nQ: What’s data skew and how do you handle it?\\nA: Occurs when certain keys have disproportionate data. Resolved using salting, repartitioning, or\\nskew join hints.\\nQ: Difference between narrow and wide transformations?\\nA: Narrow (map, filter) don’t require shuffles; wide (join, groupBy) do. Optimizing these is key for\\nlarge datasets like transactions.\\nQ: What are the Catalyst Optimizer and Tungsten?\\nA: Catalyst performs query optimization (logical and physical plans). Tungsten handles in-memory\\ncomputation and code generation for performance gains.\\n3. Delta Lake\\nQ: What is Delta Lake and why is it important?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='A: Delta Lake brings ACID transactions, schema enforcement, and versioning to data lakes —\\nensuring reliability in banking data.\\nQ: Explain time travel in Delta Lake.\\nA: Query older versions of data using timestamps or version numbers — helpful for audit and\\nrollback.\\nQ: How do you perform incremental loads?\\nA: Used MERGE INTO with watermark logic on timestamps to process only new or updated\\nrecords.\\nQ: What’s OPTIMIZE and ZORDER?\\nA: OPTIMIZE compacts small files; ZORDER colocates related data (e.g., by customer_id or\\nregion) for better performance.\\nQ: How do you handle schema evolution?\\nA: Enabled automatic schema evolution and used mergeSchema in writes to support changing data\\nstructures.\\n4. Medallion Architecture (Bronze–Silver–Gold)\\nQ: Explain the Medallion architecture.\\nA: Bronze: raw ingestion; Silver: cleaned and conformed data (via DBT); Gold: aggregated,\\nanonymized business views.\\nQ: How do you ensure data quality?\\nA: DBT tests and Databricks expectations validate data integrity. Alerts trigger on failures.\\nQ: How do you handle schema drift?\\nA: Enabled automatic schema evolution in Delta and tracked metadata changes via Unity Catalog.\\nQ: How do you implement incremental pipelines?\\nA: Used Delta change data feed (CDF) and MERGE logic to propagate only changed data between\\nlayers.\\n5. Performance & Cost Optimization\\nQ: How do you optimize Databricks jobs for cost and speed?\\nA: Used auto-scaling clusters, auto-termination, caching, and efficient partitioning.\\nQ: How do you handle large joins efficiently?\\nA: Broadcasted smaller reference data, optimized shuffle partitions, and used ZORDER for\\nco-location.\\nQ: How do you monitor performance?\\nA: Used Databricks Ganglia metrics, Spark UI, and job run history to analyze stages, shuffles, and\\ntask skew.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='6. Integrations & Orchestration\\nQ: How do you orchestrate data pipelines?\\nA: DBT handles SQL-based transformations, orchestrated via Airflow with Databricks Jobs.\\nQ: How do you integrate Databricks with BI tools?\\nA: Exposed Gold tables as Delta Live Tables or SQL endpoints for Power BI dashboards.\\nQ: How do you use Git in Databricks?\\nA: Used Databricks Repos with GitHub for CI/CD and model versioning.\\nQ: How do you handle dependencies between jobs?\\nA: Used Databricks Jobs API and Airflow DAGs to maintain execution order and dependencies.\\n7. Real-World Project Scenarios\\nQ: Describe an end-to-end pipeline you built.\\nA: Raw retail transactions ingested to Bronze; DBT transformations create Silver; Gold contains\\ndaily spend, KPIs, and de-identified analytics.\\nQ: How did you manage data privacy?\\nA: Applied masking and tokenization via DBT macros before Gold; restricted raw access via Unity\\nCatalog.\\nQ: What’s a challenge you solved in Databricks?\\nA: Solved small-file performance degradation with OPTIMIZE, ZORDER, and better partitioning.\\nQ: How do you ensure regulatory compliance?\\nA: Implemented lineage through Unity Catalog and maintained audit-ready Delta version history.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content=\"■ Bonus Quick Reference: Databricks Commands &\\nOptimization Tips\\n1. Common Delta Lake SQL Commands\\nCommand / Topic\\nExample / Description\\nCreate Delta Table\\nCREATE TABLE sales_delta USING DELTA AS SELECT * FROM parquet.`/mnt/data/sales/`;\\nUpsert (Merge)\\nMERGE INTO target t USING updates u ON t.id = u.id \\nWHEN MATCHED THEN UPDATE SET * \\nWHEN NOT MATCHED THEN INSERT *;\\nTime Travel Query\\nSELECT * FROM sales_delta VERSION AS OF 3;\\nOptimize & ZORDER\\nOPTIMIZE sales_delta ZORDER BY (customer_id);\\nVacuum\\nVACUUM sales_delta RETAIN 168 HOURS;\\n2. Useful PySpark Commands\\nCommand / Topic\\nExample / Description\\nRead Delta Table\\ndf = spark.read.format('delta').load('/mnt/data/sales_delta')\\nWrite Delta Table\\ndf.write.format('delta').mode('overwrite').save('/mnt/data/sales_delta')\\nRepartition Data\\ndf = df.repartition(50)\\nCache Table\\ndf.cache()\\nDisplay Data (Databricks)\\ndisplay(df)\\n3. Performance Optimization Tips\\nCommand / Topic\\nExample / Description\\nPartition Strategy\\nPartition large Delta tables by logical keys (e.g., region, month).\\nSmall File Problem\\nUse OPTIMIZE regularly to compact files.\\nBroadcast Joins\\nBroadcast small lookup tables using broadcast(df).\\nShuffle Optimization\\nSet spark.sql.shuffle.partitions based on data volume (e.g., 200–400).\\nSchema Evolution\\nEnable mergeSchema option for evolving data sources.\\n4. Data Governance & Security\\nCommand / Topic\\nExample / Description\\nUnity Catalog Permissions\\nGRANT SELECT ON TABLE gold.transactions TO analyst_role;\\nMask Sensitive Columns\\nUse DBT macros or CASE statements to hash/mask PII.\\nAudit Trail\\nUse Delta’s version history and Unity Catalog lineage for audits.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='■ Spark Interview Playbook (Detailed Version)\\nContext: Banking & Retail Data Pipelines with DBT & Databricks\\n■ How Spark Fits into the Banking & Retail Data Pipeline\\nData Sources (Core Banking, Retail POS, CRM, Cards) → Bronze (Raw Landing via Ingestion) → Silver\\n(Cleansed & Transformed using Spark + DBT) → Gold (Aggregated & De-identified Reporting) → Business\\nDashboards & Analytics.\\n1. Spark Core & Architecture\\nQ: How does Spark fit into your pipeline?\\nA: Spark executes the transformations defined in DBT or Databricks notebooks. It processes large volumes of\\ntransactional and customer data in a distributed manner, leveraging cluster computing.\\nQ: How does Spark handle distributed processing?\\nA: Spark splits data into partitions and processes them in parallel across executors, enabling\\nhigh-performance distributed computation.\\nQ: What Spark components do you use?\\nA: Spark SQL for structured transformations, DataFrame API for aggregations, and Structured Streaming for\\nincremental or event-driven updates.\\n2. Transformations, Actions & Use Cases\\nQ: Explain narrow vs. wide transformations.\\nA: Narrow transformations (e.g., filter, map) operate within partitions, while wide transformations (e.g.,\\ngroupBy, join) trigger shuffles across nodes.\\nQ: How do you manage large joins?\\nA: Used broadcast joins for small lookup tables, repartitioned large tables, and leveraged Delta ZORDER for\\njoin optimization.\\nQ: How do you address data skew?\\nA: Added salting keys, used skew hints, or repartitioned by balanced keys to handle uneven data distribution.\\n3. Performance Tuning & Shuffle Optimization\\nQ: Which configurations do you tune for performance?\\nA: Adjusted spark.sql.shuffle.partitions, spark.default.parallelism, and enabled autoBroadcastJoinThreshold\\nfor optimized shuffles.\\nQ: How do you handle small file issues?\\nA: Used coalesce/repartition, Delta OPTIMIZE, and periodic compaction jobs.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Q: How do you cache and persist data?\\nA: Cached frequently reused intermediate DataFrames; persisted key datasets across multiple DBT model\\nruns.\\n4. Schema Handling, Incremental Loads & Data Quality\\nQ: How do you handle schema drift?\\nA: Enabled mergeSchema for Delta tables and validated schema changes in DBT before production runs.\\nQ: How do you manage nulls and bad records?\\nA: Applied na.drop(), na.fill(), and UDF-based cleaning; directed bad records to a quarantine path for review.\\nQ: How do you perform incremental loads?\\nA: Implemented watermark-based ingestion, and Delta MERGE INTO to efficiently update only changed data.\\n5. Spark + DBT + Databricks Integration\\nQ: How do DBT models interact with Spark?\\nA: DBT compiles SQL logic that runs on Spark’s engine in Databricks. Incremental DBT models are optimized\\nusing Spark SQL for large-scale updates.\\nQ: How do you orchestrate Spark jobs?\\nA: Used Airflow or Databricks Workflows to trigger Spark notebooks and DBT models with dependency\\nmanagement.\\nQ: How do you ensure reusability?\\nA: Parameterized transformations and used modular DBT macros for shared logic across multiple Spark jobs.\\n6. Monitoring, Debugging & Real-World Scenarios\\nQ: How do you monitor Spark job performance?\\nA: Used Databricks Spark UI for stage analysis, task skew, and shuffle metrics; tracked job run durations in\\nDatabricks Job metrics.\\nQ: Example of a performance optimization you implemented?\\nA: Optimized a large aggregation from 3 hours to 45 minutes by reducing shuffle partitions, broadcasting small\\ntables, and caching intermediate data.\\nQ: How do you debug failing Spark stages?\\nA: Checked executor logs in the Spark UI, reviewed lineage in Unity Catalog, and reran partitions with lower\\nconcurrency to isolate bad data.\\n7. Governance, Security & Compliance\\nQ: How do you handle confidential data?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='A: Masked or hashed customer identifiers and ensured data encryption at rest; enforced table-level\\npermissions via Unity Catalog.\\nQ: How do you ensure lineage and traceability?\\nA: Delta Lake version history and Unity Catalog lineage tracking provide full visibility of source →\\ntransformation → output.\\nQ: What best practices do you follow for production pipelines?\\nA: Use job clusters, CI/CD deployments, parameterized notebooks, and automated validation checks in DBT +\\nSpark.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content=\"■ Bonus Quick Reference: Spark Commands & Optimization Tips\\nTopic\\nExample / Command / Description\\nRead & Write Delta\\nspark.read.format('delta').load('/mnt/sales'); df.write.format('delta').save('/mnt/output')\\nBroadcast Join\\ndf_large.join(broadcast(df_small), 'id')\\nShuffle Partitions\\nspark.conf.set('spark.sql.shuffle.partitions', 400)\\nHandle Skew\\ndf.repartition('region') or use salting technique\\nOptimize Table\\nOPTIMIZE gold.sales ZORDER BY (customer_id)\\nCache Data\\ndf.cache(); df.count()\\nSchema Evolution\\ndf.write.option('mergeSchema', 'true').format('delta').save(path)\\nIncremental Load\\nMERGE INTO gold USING updates ON keys WHEN MATCHED THEN UPDATE WHEN NOT MATCHED THEN INSERT\\nThese commands and best practices are ideal for Spark-based transformations within Databricks + DBT\\npipelines in a secure, governed banking environment.\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901c05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce09ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 8 documents into 18 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: ■ Databricks Interview Playbook\n",
      "For Experienced Data Professionals (Banking & Retail Domain)\n",
      "Comprehensive Q&A; Guide + Quick Reference for Senior-Level Interviews\n",
      "1. Databricks Core Concepts\n",
      "Q: What ...\n",
      "Metadata: {'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='■ Databricks Interview Playbook\\nFor Experienced Data Professionals (Banking & Retail Domain)\\nComprehensive Q&A; Guide + Quick Reference for Senior-Level Interviews\\n1. Databricks Core Concepts\\nQ: What is Databricks and why is it used?\\nA: Databricks is a unified analytics and AI platform built on Apache Spark that combines data\\nengineering, data science, and machine learning. In my project, we use it as a central data\\nprocessing layer to transform raw banking transactions and customer data into curated,\\nanonymized datasets for analytics.\\nQ: How does Databricks differ from traditional Spark?\\nA: Databricks provides managed clusters, collaborative notebooks, built-in Delta Lake, and\\noptimized I/O performance, improving reliability compared to self-managed Spark.\\nQ: What is Unity Catalog and its role?\\nA: Unity Catalog provides centralized governance — access control, data lineage, and audit\\nlogging — across workspaces. We used it to manage permissions on de-identified retail data and'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='A: Unity Catalog provides centralized governance — access control, data lineage, and audit\\nlogging — across workspaces. We used it to manage permissions on de-identified retail data and\\ntrack lineage across DBT models and reporting schemas.\\n2. Apache Spark on Databricks\\nQ: How do you optimize Spark jobs in Databricks?\\nA: Adjusted shuffle partitions, used broadcast joins for smaller tables, persisted intermediate data,\\nand monitored via Spark UI.\\nQ: What’s data skew and how do you handle it?\\nA: Occurs when certain keys have disproportionate data. Resolved using salting, repartitioning, or\\nskew join hints.\\nQ: Difference between narrow and wide transformations?\\nA: Narrow (map, filter) don’t require shuffles; wide (join, groupBy) do. Optimizing these is key for\\nlarge datasets like transactions.\\nQ: What are the Catalyst Optimizer and Tungsten?\\nA: Catalyst performs query optimization (logical and physical plans). Tungsten handles in-memory'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='large datasets like transactions.\\nQ: What are the Catalyst Optimizer and Tungsten?\\nA: Catalyst performs query optimization (logical and physical plans). Tungsten handles in-memory\\ncomputation and code generation for performance gains.\\n3. Delta Lake\\nQ: What is Delta Lake and why is it important?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='A: Delta Lake brings ACID transactions, schema enforcement, and versioning to data lakes —\\nensuring reliability in banking data.\\nQ: Explain time travel in Delta Lake.\\nA: Query older versions of data using timestamps or version numbers — helpful for audit and\\nrollback.\\nQ: How do you perform incremental loads?\\nA: Used MERGE INTO with watermark logic on timestamps to process only new or updated\\nrecords.\\nQ: What’s OPTIMIZE and ZORDER?\\nA: OPTIMIZE compacts small files; ZORDER colocates related data (e.g., by customer_id or\\nregion) for better performance.\\nQ: How do you handle schema evolution?\\nA: Enabled automatic schema evolution and used mergeSchema in writes to support changing data\\nstructures.\\n4. Medallion Architecture (Bronze–Silver–Gold)\\nQ: Explain the Medallion architecture.\\nA: Bronze: raw ingestion; Silver: cleaned and conformed data (via DBT); Gold: aggregated,\\nanonymized business views.\\nQ: How do you ensure data quality?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Q: Explain the Medallion architecture.\\nA: Bronze: raw ingestion; Silver: cleaned and conformed data (via DBT); Gold: aggregated,\\nanonymized business views.\\nQ: How do you ensure data quality?\\nA: DBT tests and Databricks expectations validate data integrity. Alerts trigger on failures.\\nQ: How do you handle schema drift?\\nA: Enabled automatic schema evolution in Delta and tracked metadata changes via Unity Catalog.\\nQ: How do you implement incremental pipelines?\\nA: Used Delta change data feed (CDF) and MERGE logic to propagate only changed data between\\nlayers.\\n5. Performance & Cost Optimization\\nQ: How do you optimize Databricks jobs for cost and speed?\\nA: Used auto-scaling clusters, auto-termination, caching, and efficient partitioning.\\nQ: How do you handle large joins efficiently?\\nA: Broadcasted smaller reference data, optimized shuffle partitions, and used ZORDER for\\nco-location.\\nQ: How do you monitor performance?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Q: How do you handle large joins efficiently?\\nA: Broadcasted smaller reference data, optimized shuffle partitions, and used ZORDER for\\nco-location.\\nQ: How do you monitor performance?\\nA: Used Databricks Ganglia metrics, Spark UI, and job run history to analyze stages, shuffles, and\\ntask skew.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='6. Integrations & Orchestration\\nQ: How do you orchestrate data pipelines?\\nA: DBT handles SQL-based transformations, orchestrated via Airflow with Databricks Jobs.\\nQ: How do you integrate Databricks with BI tools?\\nA: Exposed Gold tables as Delta Live Tables or SQL endpoints for Power BI dashboards.\\nQ: How do you use Git in Databricks?\\nA: Used Databricks Repos with GitHub for CI/CD and model versioning.\\nQ: How do you handle dependencies between jobs?\\nA: Used Databricks Jobs API and Airflow DAGs to maintain execution order and dependencies.\\n7. Real-World Project Scenarios\\nQ: Describe an end-to-end pipeline you built.\\nA: Raw retail transactions ingested to Bronze; DBT transformations create Silver; Gold contains\\ndaily spend, KPIs, and de-identified analytics.\\nQ: How did you manage data privacy?\\nA: Applied masking and tokenization via DBT macros before Gold; restricted raw access via Unity\\nCatalog.\\nQ: What’s a challenge you solved in Databricks?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Q: How did you manage data privacy?\\nA: Applied masking and tokenization via DBT macros before Gold; restricted raw access via Unity\\nCatalog.\\nQ: What’s a challenge you solved in Databricks?\\nA: Solved small-file performance degradation with OPTIMIZE, ZORDER, and better partitioning.\\nQ: How do you ensure regulatory compliance?\\nA: Implemented lineage through Unity Catalog and maintained audit-ready Delta version history.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content=\"■ Bonus Quick Reference: Databricks Commands &\\nOptimization Tips\\n1. Common Delta Lake SQL Commands\\nCommand / Topic\\nExample / Description\\nCreate Delta Table\\nCREATE TABLE sales_delta USING DELTA AS SELECT * FROM parquet.`/mnt/data/sales/`;\\nUpsert (Merge)\\nMERGE INTO target t USING updates u ON t.id = u.id \\nWHEN MATCHED THEN UPDATE SET * \\nWHEN NOT MATCHED THEN INSERT *;\\nTime Travel Query\\nSELECT * FROM sales_delta VERSION AS OF 3;\\nOptimize & ZORDER\\nOPTIMIZE sales_delta ZORDER BY (customer_id);\\nVacuum\\nVACUUM sales_delta RETAIN 168 HOURS;\\n2. Useful PySpark Commands\\nCommand / Topic\\nExample / Description\\nRead Delta Table\\ndf = spark.read.format('delta').load('/mnt/data/sales_delta')\\nWrite Delta Table\\ndf.write.format('delta').mode('overwrite').save('/mnt/data/sales_delta')\\nRepartition Data\\ndf = df.repartition(50)\\nCache Table\\ndf.cache()\\nDisplay Data (Databricks)\\ndisplay(df)\\n3. Performance Optimization Tips\\nCommand / Topic\\nExample / Description\\nPartition Strategy\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T16:26:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T16:26:25+00:00', 'subject': '(unspecified)', 'title': 'Databricks Interview Playbook - Full Edition', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'Databricks_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Repartition Data\\ndf = df.repartition(50)\\nCache Table\\ndf.cache()\\nDisplay Data (Databricks)\\ndisplay(df)\\n3. Performance Optimization Tips\\nCommand / Topic\\nExample / Description\\nPartition Strategy\\nPartition large Delta tables by logical keys (e.g., region, month).\\nSmall File Problem\\nUse OPTIMIZE regularly to compact files.\\nBroadcast Joins\\nBroadcast small lookup tables using broadcast(df).\\nShuffle Optimization\\nSet spark.sql.shuffle.partitions based on data volume (e.g., 200–400).\\nSchema Evolution\\nEnable mergeSchema option for evolving data sources.\\n4. Data Governance & Security\\nCommand / Topic\\nExample / Description\\nUnity Catalog Permissions\\nGRANT SELECT ON TABLE gold.transactions TO analyst_role;\\nMask Sensitive Columns\\nUse DBT macros or CASE statements to hash/mask PII.\\nAudit Trail\\nUse Delta’s version history and Unity Catalog lineage for audits.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='■ Spark Interview Playbook (Detailed Version)\\nContext: Banking & Retail Data Pipelines with DBT & Databricks\\n■ How Spark Fits into the Banking & Retail Data Pipeline\\nData Sources (Core Banking, Retail POS, CRM, Cards) → Bronze (Raw Landing via Ingestion) → Silver\\n(Cleansed & Transformed using Spark + DBT) → Gold (Aggregated & De-identified Reporting) → Business\\nDashboards & Analytics.\\n1. Spark Core & Architecture\\nQ: How does Spark fit into your pipeline?\\nA: Spark executes the transformations defined in DBT or Databricks notebooks. It processes large volumes of\\ntransactional and customer data in a distributed manner, leveraging cluster computing.\\nQ: How does Spark handle distributed processing?\\nA: Spark splits data into partitions and processes them in parallel across executors, enabling\\nhigh-performance distributed computation.\\nQ: What Spark components do you use?\\nA: Spark SQL for structured transformations, DataFrame API for aggregations, and Structured Streaming for'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='high-performance distributed computation.\\nQ: What Spark components do you use?\\nA: Spark SQL for structured transformations, DataFrame API for aggregations, and Structured Streaming for\\nincremental or event-driven updates.\\n2. Transformations, Actions & Use Cases\\nQ: Explain narrow vs. wide transformations.\\nA: Narrow transformations (e.g., filter, map) operate within partitions, while wide transformations (e.g.,\\ngroupBy, join) trigger shuffles across nodes.\\nQ: How do you manage large joins?\\nA: Used broadcast joins for small lookup tables, repartitioned large tables, and leveraged Delta ZORDER for\\njoin optimization.\\nQ: How do you address data skew?\\nA: Added salting keys, used skew hints, or repartitioned by balanced keys to handle uneven data distribution.\\n3. Performance Tuning & Shuffle Optimization\\nQ: Which configurations do you tune for performance?\\nA: Adjusted spark.sql.shuffle.partitions, spark.default.parallelism, and enabled autoBroadcastJoinThreshold\\nfor optimized shuffles.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Q: Which configurations do you tune for performance?\\nA: Adjusted spark.sql.shuffle.partitions, spark.default.parallelism, and enabled autoBroadcastJoinThreshold\\nfor optimized shuffles.\\nQ: How do you handle small file issues?\\nA: Used coalesce/repartition, Delta OPTIMIZE, and periodic compaction jobs.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='Q: How do you cache and persist data?\\nA: Cached frequently reused intermediate DataFrames; persisted key datasets across multiple DBT model\\nruns.\\n4. Schema Handling, Incremental Loads & Data Quality\\nQ: How do you handle schema drift?\\nA: Enabled mergeSchema for Delta tables and validated schema changes in DBT before production runs.\\nQ: How do you manage nulls and bad records?\\nA: Applied na.drop(), na.fill(), and UDF-based cleaning; directed bad records to a quarantine path for review.\\nQ: How do you perform incremental loads?\\nA: Implemented watermark-based ingestion, and Delta MERGE INTO to efficiently update only changed data.\\n5. Spark + DBT + Databricks Integration\\nQ: How do DBT models interact with Spark?\\nA: DBT compiles SQL logic that runs on Spark’s engine in Databricks. Incremental DBT models are optimized\\nusing Spark SQL for large-scale updates.\\nQ: How do you orchestrate Spark jobs?\\nA: Used Airflow or Databricks Workflows to trigger Spark notebooks and DBT models with dependency'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='using Spark SQL for large-scale updates.\\nQ: How do you orchestrate Spark jobs?\\nA: Used Airflow or Databricks Workflows to trigger Spark notebooks and DBT models with dependency\\nmanagement.\\nQ: How do you ensure reusability?\\nA: Parameterized transformations and used modular DBT macros for shared logic across multiple Spark jobs.\\n6. Monitoring, Debugging & Real-World Scenarios\\nQ: How do you monitor Spark job performance?\\nA: Used Databricks Spark UI for stage analysis, task skew, and shuffle metrics; tracked job run durations in\\nDatabricks Job metrics.\\nQ: Example of a performance optimization you implemented?\\nA: Optimized a large aggregation from 3 hours to 45 minutes by reducing shuffle partitions, broadcasting small\\ntables, and caching intermediate data.\\nQ: How do you debug failing Spark stages?\\nA: Checked executor logs in the Spark UI, reviewed lineage in Unity Catalog, and reran partitions with lower\\nconcurrency to isolate bad data.\\n7. Governance, Security & Compliance'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='A: Checked executor logs in the Spark UI, reviewed lineage in Unity Catalog, and reran partitions with lower\\nconcurrency to isolate bad data.\\n7. Governance, Security & Compliance\\nQ: How do you handle confidential data?'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content='A: Masked or hashed customer identifiers and ensured data encryption at rest; enforced table-level\\npermissions via Unity Catalog.\\nQ: How do you ensure lineage and traceability?\\nA: Delta Lake version history and Unity Catalog lineage tracking provide full visibility of source →\\ntransformation → output.\\nQ: What best practices do you follow for production pipelines?\\nA: Use job clusters, CI/CD deployments, parameterized notebooks, and automated validation checks in DBT +\\nSpark.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-24T17:56:57+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-24T17:56:57+00:00', 'subject': '(unspecified)', 'title': 'Spark Interview Playbook - Full Version', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\Spark_Interview_Playbook_Full.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'Spark_Interview_Playbook_Full.pdf', 'file_type': 'pdf'}, page_content=\"■ Bonus Quick Reference: Spark Commands & Optimization Tips\\nTopic\\nExample / Command / Description\\nRead & Write Delta\\nspark.read.format('delta').load('/mnt/sales'); df.write.format('delta').save('/mnt/output')\\nBroadcast Join\\ndf_large.join(broadcast(df_small), 'id')\\nShuffle Partitions\\nspark.conf.set('spark.sql.shuffle.partitions', 400)\\nHandle Skew\\ndf.repartition('region') or use salting technique\\nOptimize Table\\nOPTIMIZE gold.sales ZORDER BY (customer_id)\\nCache Data\\ndf.cache(); df.count()\\nSchema Evolution\\ndf.write.option('mergeSchema', 'true').format('delta').save(path)\\nIncremental Load\\nMERGE INTO gold USING updates ON keys WHEN MATCHED THEN UPDATE WHEN NOT MATCHED THEN INSERT\\nThese commands and best practices are ideal for Spark-based transformations within Databricks + DBT\\npipelines in a secure, governed banking environment.\")]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0e4b1",
   "metadata": {},
   "source": [
    "embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # type: ignore\n",
    "import chromadb # type: ignore\n",
    "from chromadb.config import Settings # type: ignore\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33782553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\YTRAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ansum\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1e1e9175210>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c765fd8",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bd0c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1e1e917ef90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0207fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 18 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (18, 384)\n",
      "Adding 18 documents to vector store...\n",
      "Successfully added 18 documents to vector store\n",
      "Total documents in collection: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40be9b",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b7947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91b5dc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1e1e95060d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60c0aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is databricks and why you need it'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_d051770e_0',\n",
       "  'content': '■ Databricks Interview Playbook\\nFor Experienced Data Professionals (Banking & Retail Domain)\\nComprehensive Q&A; Guide + Quick Reference for Senior-Level Interviews\\n1. Databricks Core Concepts\\nQ: What is Databricks and why is it used?\\nA: Databricks is a unified analytics and AI platform built on Apache Spark that combines data\\nengineering, data science, and machine learning. In my project, we use it as a central data\\nprocessing layer to transform raw banking transactions and customer data into curated,\\nanonymized datasets for analytics.\\nQ: How does Databricks differ from traditional Spark?\\nA: Databricks provides managed clusters, collaborative notebooks, built-in Delta Lake, and\\noptimized I/O performance, improving reliability compared to self-managed Spark.\\nQ: What is Unity Catalog and its role?\\nA: Unity Catalog provides centralized governance — access control, data lineage, and audit\\nlogging — across workspaces. We used it to manage permissions on de-identified retail data and',\n",
       "  'metadata': {'author': '(anonymous)',\n",
       "   'content_length': 992,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'keywords': '',\n",
       "   'title': 'Databricks Interview Playbook - Full Edition',\n",
       "   'page': 0,\n",
       "   'source_file': 'Databricks_Interview_Playbook_Full.pdf',\n",
       "   'page_label': '1',\n",
       "   'creator': '(unspecified)',\n",
       "   'creationdate': '2025-10-24T16:26:25+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 4,\n",
       "   'subject': '(unspecified)',\n",
       "   'doc_index': 0,\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2025-10-24T16:26:25+00:00'},\n",
       "  'similarity_score': 0.26072120666503906,\n",
       "  'distance': 0.7392787933349609,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_6e614d1c_6',\n",
       "  'content': '6. Integrations & Orchestration\\nQ: How do you orchestrate data pipelines?\\nA: DBT handles SQL-based transformations, orchestrated via Airflow with Databricks Jobs.\\nQ: How do you integrate Databricks with BI tools?\\nA: Exposed Gold tables as Delta Live Tables or SQL endpoints for Power BI dashboards.\\nQ: How do you use Git in Databricks?\\nA: Used Databricks Repos with GitHub for CI/CD and model versioning.\\nQ: How do you handle dependencies between jobs?\\nA: Used Databricks Jobs API and Airflow DAGs to maintain execution order and dependencies.\\n7. Real-World Project Scenarios\\nQ: Describe an end-to-end pipeline you built.\\nA: Raw retail transactions ingested to Bronze; DBT transformations create Silver; Gold contains\\ndaily spend, KPIs, and de-identified analytics.\\nQ: How did you manage data privacy?\\nA: Applied masking and tokenization via DBT macros before Gold; restricted raw access via Unity\\nCatalog.\\nQ: What’s a challenge you solved in Databricks?',\n",
       "  'metadata': {'doc_index': 6,\n",
       "   'total_pages': 4,\n",
       "   'author': '(anonymous)',\n",
       "   'page_label': '3',\n",
       "   'page': 2,\n",
       "   'creator': '(unspecified)',\n",
       "   'moddate': '2025-10-24T16:26:25+00:00',\n",
       "   'keywords': '',\n",
       "   'creationdate': '2025-10-24T16:26:25+00:00',\n",
       "   'subject': '(unspecified)',\n",
       "   'trapped': '/False',\n",
       "   'content_length': 954,\n",
       "   'title': 'Databricks Interview Playbook - Full Edition',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf',\n",
       "   'source_file': 'Databricks_Interview_Playbook_Full.pdf',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com'},\n",
       "  'similarity_score': 0.03446751832962036,\n",
       "  'distance': 0.9655324816703796,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_7ab6d4f4_7',\n",
       "  'content': 'Q: How did you manage data privacy?\\nA: Applied masking and tokenization via DBT macros before Gold; restricted raw access via Unity\\nCatalog.\\nQ: What’s a challenge you solved in Databricks?\\nA: Solved small-file performance degradation with OPTIMIZE, ZORDER, and better partitioning.\\nQ: How do you ensure regulatory compliance?\\nA: Implemented lineage through Unity Catalog and maintained audit-ready Delta version history.',\n",
       "  'metadata': {'creator': '(unspecified)',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Databricks_Interview_Playbook_Full.pdf',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'page': 2,\n",
       "   'doc_index': 7,\n",
       "   'author': '(anonymous)',\n",
       "   'creationdate': '2025-10-24T16:26:25+00:00',\n",
       "   'source_file': 'Databricks_Interview_Playbook_Full.pdf',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2025-10-24T16:26:25+00:00',\n",
       "   'title': 'Databricks Interview Playbook - Full Edition',\n",
       "   'subject': '(unspecified)',\n",
       "   'total_pages': 4,\n",
       "   'content_length': 420,\n",
       "   'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '3'},\n",
       "  'similarity_score': 0.007513999938964844,\n",
       "  'distance': 0.9924860000610352,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is databricks and why you need it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq # type: ignore\n",
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60d9a2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is databricks?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks is a unified analytics and AI platform built on Apache Spark that combines data engineering, data science, and machine learning.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is databricks?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f562495",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f671a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Delta Lake'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Delta Lake\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
